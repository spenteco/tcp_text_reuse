{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tests for matching_functions, part I\n",
    "\n",
    "\"Test\" perhaps not an exact term for what's going on in this notebook; instead, it might be better to say that this notebook demonstrates how morphadorned EEBO-TCP data is transformed into data suitable for finding a certain kind of text reuse.\n",
    "\n",
    "The goal is to create for each EEBO-TCP file (or, in the case of Biblical texts, for each verse) data like:\n",
    "\n",
    "    {'tokens': ['25', ' ', '¶', ' ', 'And', ' ', 'Adam', ' ', '•new', ' ', 'his', ' ', 'wife', ' ', 'again',\n",
    "        ',', ' ', 'and', ' ', 'she', ' ', '〈◊〉', ' ', 'a', ' ', 'son', ',', ' ', 'and', ' ', 'called', ' ',\n",
    "        'his', ' ', 'name', ' ', 'Seth', ':', ' ', 'For', ' ', 'God', ',', ' ', 'said', ' ', 'she', ',', ' ',\n",
    "        'hath', ' ', 'appo•••••', ' ', 'me', ' ', 'another', ' ', 'seed', ' ', 'in', ' ', 'stead', ' ', 'of', \n",
    "        ' ', 'Abel', ',', ' ', 'whom', ' ', 'Cain', ' ', 'slew', '.', '8', ' ', 'And', ' ', 'Abraham', ' ',\n",
    "        'said', ',', ' ', 'My', ' ', 'son', ',', ' ', 'God', ' ', 'will', ' ', 'provide', ' ', 'himself', ' ',\n",
    "        'a', ' ', 'lamb', ' ', 'for', ' ', 'a', ' ', 'burnt-offering', ':', ' ', 'so', ' ', 'they', ' ',\n",
    "        'went', ' ', 'both', ' ', 'of', ' ', 'them', ' ', 'together', '.'], \n",
    "    'lemmas': [' ', ' ', ' ', ' ', ' ', ' ', 'adam', ' ', ' ', ' ', ' ', ' ', 'wife', ' ', ' ', ' ', ' ', \n",
    "        ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', 'son', ' ', ' ', ' ', ' ', 'call', ' ', ' ', ' ', 'name', \n",
    "        ' ', 'seth', ' ', ' ', ' ', ' ', 'god', ' ', ' ', 'say', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', \n",
    "        ' ', ' ', 'another', ' ', 'seed', ' ', ' ', ' ', 'stead', ' ', ' ', ' ', 'abel', ' ', ' ', ' ', \n",
    "        ' ', 'cain', ' ', 'slay', ' ', ' ', ' ', ' ', ' ', 'abraham', ' ', 'say', ' ', ' ', ' ', ' ', 'son', \n",
    "        ' ', ' ', 'god', ' ', ' ', ' ', 'provide', ' ', ' ', ' ', ' ', ' ', 'lamb', ' ', ' ', ' ', ' ', ' ', \n",
    "        ' ', ' ', ' ', ' ', ' ', ' ', ' ', 'go', ' ', ' ', ' ', ' ', ' ', ' ', ' ', 'together', ' '], \n",
    "    'non_space_lemmas': ['adam', 'wife', 'son', 'call', 'name', 'seth', 'god', 'say', 'another', 'seed',\n",
    "        'stead', 'abel', 'cain', 'slay', 'abraham', 'say', 'son', 'god', 'provide', 'lamb', 'go', 'together'], \n",
    "    'offsets': [6, 12, 25, 30, 34, 36, 41, 44, 55, 57, 61, 65, 70, 72, 78, 80, 85, 88, 92, 98, 111, 119], \n",
    "    'shingles': {('adam', 'wife', 'son'): [[0, 2]], ('wife', 'son', 'call'): [[1, 3]], \n",
    "        ('son', 'call', 'name'): [[2, 4]], ('call', 'name', 'seth'): [[3, 5]], \n",
    "        ('name', 'seth', 'god'): [[4, 6]], ('seth', 'god', 'say'): [[5, 7]], \n",
    "        ('god', 'say', 'another'): [[6, 8]], ('say', 'another', 'seed'): [[7, 9]], \n",
    "        ('another', 'seed', 'stead'): [[8, 10]], ('seed', 'stead', 'abel'): [[9, 11]], \n",
    "        ('stead', 'abel', 'cain'): [[10, 12]], ('abel', 'cain', 'slay'): [[11, 13]], \n",
    "        ('cain', 'slay', 'abraham'): [[12, 14]], ('slay', 'abraham', 'say'): [[13, 15]], \n",
    "        ('abraham', 'say', 'son'): [[14, 16]], ('say', 'son', 'god'): [[15, 17]], \n",
    "        ('son', 'god', 'provide'): [[16, 18]], ('god', 'provide', 'lamb'): [[17, 19]], \n",
    "        ('provide', 'lamb', 'go'): [[18, 20]], ('lamb', 'go', 'together'): [[19, 21]]}}\n",
    "        \n",
    "The data is a dictionary with 5 key-value pairs:\n",
    "\n",
    "* **tokens** contains the sequence of words (original spelling), spaces and punctuation from the corresponding morphadorned EEBO-TCP file; we preserve it so that, when we find a match between two texts, we can reconstruction the full passages.\n",
    "* **lemmas** contains the sequence of lemmas which correspond with the tokens; certain lemmas (stop words, punctuation, numbers, lemma made up partially or entirely of non-latin alphabets) are replaced with spaces.\n",
    "* **non_space_lemmas** contains the same data as lemmas, less spaces.\n",
    "* **offsets** contains the position in lemmas and tokens of every entry in non_space_lemma.  E.g. \"adam\", the first entry in non_space_lemma, corresponds with position 6 in tokens (we start counting with 0), \"wife\" with 12.\n",
    "* **shingles** contains ngrams from non_space_lemmas with a list of their corresponding starting and stopping position in offsets.  E.g., \"('adam', 'wife', 'son')\" starts at 0 in offsets and stops at 2.  This makes it possible to work backward from a shingle to a fuller passage in the original text.  If we start here:\n",
    "\n",
    "    ('adam', 'wife', 'son'): \\[\\[0, 2\\]\\]\n",
    "    \n",
    "The value at position 0 in offsets is 6, which points to a starting position in tokens (\"Adam\").  The value at position 2 in offsets is 12, which points to an ending location in tokens (\"son\").  Everything between the two constitutes the original text which corresponds with the sequence:\n",
    "\n",
    "    'And', ' ', 'Adam', ' ', '•new', ' ', 'his', ' ', 'wife', ' ', 'again', ',', ' ', 'and', ' ', 'she', \n",
    "    ' ', '〈◊〉', ' ', 'a', ' ', 'son'\n",
    "    \n",
    "It's a lot of bother, of course.  But it's worth it because it is ridiculously fast to compare two texts' shingles.  Which makes the whole \"find all the Bible quotations\" thing possible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matching_functions import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metadata\n",
    "\n",
    "How we load and search metadata in these notebooks . . . "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A43441 {'year': '1648', 'author': 'Herrick, Robert, 1591-1674.|Marshall, William, fl. 1617-1650.', 'title': 'Hesperides, or, The works both humane & divine of Robert Herrick, Esq.'}\n"
     ]
    }
   ],
   "source": [
    "metadata = load_metadata('metadata/EEBO_metadata.tsv')\n",
    "\n",
    "for k, v in metadata.items():\n",
    "    if 'Herrick' in v['author'] and v['year'] == '1648':\n",
    "        print(k, v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select useful lemma\n",
    "\n",
    "In the morphadorned output, lemma attributes can contain all sorts of things: gap markers, words written in non-latin alphabets, astrological symbols, etc.  We want to filter all of that out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tree True\n",
      "Tree True\n",
      "None False\n",
      "Tree! False\n"
     ]
    }
   ],
   "source": [
    "print('tree', is_lemma_valid('tree'))\n",
    "print('Tree', is_lemma_valid('Tree'))\n",
    "print(None, is_lemma_valid(None))\n",
    "print('Tree!', is_lemma_valid('Tree!'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### get_one_file . . . \n",
    "\n",
    "Demonstrate the process for reading and performing the first set of transformations to a morphadorned EEBP-TCP file.\n",
    " \n",
    "     get_tokens_for_iterator\n",
    "     select_token_and_lemma\n",
    "     is_lemma_valid\n",
    "     \n",
    "### The test file contains 121 . . . \n",
    "\n",
    " . . . words, spaces, and punctuation marks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "121\r\n"
     ]
    }
   ],
   "source": [
    "!egrep '<pc|<c|<w' test_adorned_xml/test_get_one_file.xml | wc -l"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The actual process and results\n",
    "\n",
    "Note that\n",
    "\n",
    "1.  We get 121 tokens and 121 lemma, which is the right number.\n",
    "2.  Spaces count as tokens, as do punctutation.\n",
    "3.  Lemma consists of non-stopword, valid lemma (see the section above \"Select useful lemma\").  Note, however, that when a lemma is not considered \"valid\" or \"useful\", we replace that lemma with a space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(tokens) 121 len(lemmas) 121\n",
      "\n",
      "['25', ' ', '¶', ' ', 'And', ' ', 'Adam', ' ', '•new', ' ', 'his', ' ', 'wife', ' ', 'again', ',', ' ', 'and', ' ', 'she', ' ', '〈◊〉', ' ', 'a', ' ', 'son', ',', ' ', 'and', ' ', 'called', ' ', 'his', ' ', 'name', ' ', 'Seth', ':', ' ', 'For', ' ', 'God', ',', ' ', 'said', ' ', 'she', ',', ' ', 'hath', ' ', 'appo•••••', ' ', 'me', ' ', 'another', ' ', 'seed', ' ', 'in', ' ', 'stead', ' ', 'of', ' ', 'Abel', ',', ' ', 'whom', ' ', 'Cain', ' ', 'slew', '.', '8', ' ', 'And', ' ', 'Abraham', ' ', 'said', ',', ' ', 'My', ' ', 'son', ',', ' ', 'God', ' ', 'will', ' ', 'provide', ' ', 'himself', ' ', 'a', ' ', 'lamb', ' ', 'for', ' ', 'a', ' ', 'burnt-offering', ':', ' ', 'so', ' ', 'they', ' ', 'went', ' ', 'both', ' ', 'of', ' ', 'them', ' ', 'together', '.']\n",
      "\n",
      "[' ', ' ', ' ', ' ', ' ', ' ', 'adam', ' ', ' ', ' ', ' ', ' ', 'wife', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', 'son', ' ', ' ', ' ', ' ', 'call', ' ', ' ', ' ', 'name', ' ', 'seth', ' ', ' ', ' ', ' ', 'god', ' ', ' ', 'say', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', 'another', ' ', 'seed', ' ', ' ', ' ', 'stead', ' ', ' ', ' ', 'abel', ' ', ' ', ' ', ' ', 'cain', ' ', 'slay', ' ', ' ', ' ', ' ', ' ', 'abraham', ' ', 'say', ' ', ' ', ' ', ' ', 'son', ' ', ' ', 'god', ' ', ' ', ' ', 'provide', ' ', ' ', ' ', ' ', ' ', 'lamb', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', 'go', ' ', ' ', ' ', ' ', ' ', ' ', ' ', 'together', ' ']\n"
     ]
    }
   ],
   "source": [
    "tokens, lemmas = tokenize_lemmatize_one_file('test_adorned_xml/test_get_one_file.xml')\n",
    "\n",
    "print('len(tokens)', len(tokens), 'len(lemmas)', len(lemmas))\n",
    "print()\n",
    "print(tokens)\n",
    "print()\n",
    "print(lemmas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  The shingle building process\n",
    "\n",
    "First step in the shingle building process is the creation of two lists: one, of non-space lemmas; the other, of the locations of those non-space lemma in the tokens and lemmas lists created in the preceeding step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(non_space_lemmas) 22 len(offsets) 22\n",
      "\n",
      "['adam', 'wife', 'son', 'call', 'name', 'seth', 'god', 'say', 'another', 'seed', 'stead', 'abel', 'cain', 'slay', 'abraham', 'say', 'son', 'god', 'provide', 'lamb', 'go', 'together']\n",
      "\n",
      "[6, 12, 25, 30, 34, 36, 41, 44, 55, 57, 61, 65, 70, 72, 78, 80, 85, 88, 92, 98, 111, 119]\n"
     ]
    }
   ],
   "source": [
    "non_space_lemmas, offsets = get_non_space_lemmas_and_offsets(lemmas)\n",
    "\n",
    "print('len(non_space_lemmas)', len(non_space_lemmas), 'len(offsets)', len(offsets))\n",
    "print()\n",
    "print(non_space_lemmas)\n",
    "print()\n",
    "print(offsets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Shingles** are derived from non_space_lemmas and offsets.  A shingle consists of:\n",
    "\n",
    "1.  An n-gram non_space_lemmas from non-space lemma;\n",
    "2.  A list of starting and ending positions within the offsets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(shingles) 20\n",
      "\n",
      "{('adam', 'wife', 'son'): [[0, 2]], ('wife', 'son', 'call'): [[1, 3]], ('son', 'call', 'name'): [[2, 4]], ('call', 'name', 'seth'): [[3, 5]], ('name', 'seth', 'god'): [[4, 6]], ('seth', 'god', 'say'): [[5, 7]], ('god', 'say', 'another'): [[6, 8]], ('say', 'another', 'seed'): [[7, 9]], ('another', 'seed', 'stead'): [[8, 10]], ('seed', 'stead', 'abel'): [[9, 11]], ('stead', 'abel', 'cain'): [[10, 12]], ('abel', 'cain', 'slay'): [[11, 13]], ('cain', 'slay', 'abraham'): [[12, 14]], ('slay', 'abraham', 'say'): [[13, 15]], ('abraham', 'say', 'son'): [[14, 16]], ('say', 'son', 'god'): [[15, 17]], ('son', 'god', 'provide'): [[16, 18]], ('god', 'provide', 'lamb'): [[17, 19]], ('provide', 'lamb', 'go'): [[18, 20]], ('lamb', 'go', 'together'): [[19, 21]]}\n"
     ]
    }
   ],
   "source": [
    "SHINGLE_LENGTH = 3\n",
    "\n",
    "shingles = shingle_tokens(non_space_lemmas, SHINGLE_LENGTH)\n",
    "\n",
    "print('len(shingles)', len(shingles))\n",
    "print()\n",
    "print(shingles)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Serialize\n",
    "\n",
    "Basic code to save a file's data to disk, and to load it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'tokens': ['25', ' ', '¶', ' ', 'And', ' ', 'Adam', ' ', '•new', ' ', 'his', ' ', 'wife', ' ', 'again', ',', ' ', 'and', ' ', 'she', ' ', '〈◊〉', ' ', 'a', ' ', 'son', ',', ' ', 'and', ' ', 'called', ' ', 'his', ' ', 'name', ' ', 'Seth', ':', ' ', 'For', ' ', 'God', ',', ' ', 'said', ' ', 'she', ',', ' ', 'hath', ' ', 'appo•••••', ' ', 'me', ' ', 'another', ' ', 'seed', ' ', 'in', ' ', 'stead', ' ', 'of', ' ', 'Abel', ',', ' ', 'whom', ' ', 'Cain', ' ', 'slew', '.', '8', ' ', 'And', ' ', 'Abraham', ' ', 'said', ',', ' ', 'My', ' ', 'son', ',', ' ', 'God', ' ', 'will', ' ', 'provide', ' ', 'himself', ' ', 'a', ' ', 'lamb', ' ', 'for', ' ', 'a', ' ', 'burnt-offering', ':', ' ', 'so', ' ', 'they', ' ', 'went', ' ', 'both', ' ', 'of', ' ', 'them', ' ', 'together', '.'], 'lemmas': [' ', ' ', ' ', ' ', ' ', ' ', 'adam', ' ', ' ', ' ', ' ', ' ', 'wife', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', 'son', ' ', ' ', ' ', ' ', 'call', ' ', ' ', ' ', 'name', ' ', 'seth', ' ', ' ', ' ', ' ', 'god', ' ', ' ', 'say', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', 'another', ' ', 'seed', ' ', ' ', ' ', 'stead', ' ', ' ', ' ', 'abel', ' ', ' ', ' ', ' ', 'cain', ' ', 'slay', ' ', ' ', ' ', ' ', ' ', 'abraham', ' ', 'say', ' ', ' ', ' ', ' ', 'son', ' ', ' ', 'god', ' ', ' ', ' ', 'provide', ' ', ' ', ' ', ' ', ' ', 'lamb', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', 'go', ' ', ' ', ' ', ' ', ' ', ' ', ' ', 'together', ' '], 'non_space_lemmas': ['adam', 'wife', 'son', 'call', 'name', 'seth', 'god', 'say', 'another', 'seed', 'stead', 'abel', 'cain', 'slay', 'abraham', 'say', 'son', 'god', 'provide', 'lamb', 'go', 'together'], 'offsets': [6, 12, 25, 30, 34, 36, 41, 44, 55, 57, 61, 65, 70, 72, 78, 80, 85, 88, 92, 98, 111, 119], 'shingles': {('adam', 'wife', 'son'): [[0, 2]], ('wife', 'son', 'call'): [[1, 3]], ('son', 'call', 'name'): [[2, 4]], ('call', 'name', 'seth'): [[3, 5]], ('name', 'seth', 'god'): [[4, 6]], ('seth', 'god', 'say'): [[5, 7]], ('god', 'say', 'another'): [[6, 8]], ('say', 'another', 'seed'): [[7, 9]], ('another', 'seed', 'stead'): [[8, 10]], ('seed', 'stead', 'abel'): [[9, 11]], ('stead', 'abel', 'cain'): [[10, 12]], ('abel', 'cain', 'slay'): [[11, 13]], ('cain', 'slay', 'abraham'): [[12, 14]], ('slay', 'abraham', 'say'): [[13, 15]], ('abraham', 'say', 'son'): [[14, 16]], ('say', 'son', 'god'): [[15, 17]], ('son', 'god', 'provide'): [[16, 18]], ('god', 'provide', 'lamb'): [[17, 19]], ('provide', 'lamb', 'go'): [[18, 20]], ('lamb', 'go', 'together'): [[19, 21]]}}\n"
     ]
    }
   ],
   "source": [
    "file_a_contents = {'tokens': tokens, 'lemmas': lemmas,\n",
    "                      'non_space_lemmas': non_space_lemmas, 'offsets': offsets,\n",
    "                      'shingles': shingles}\n",
    "\n",
    "f = open('test_pickles/test_get_one_file.pickle', 'wb')\n",
    "pickle.dump(file_a_contents, f)\n",
    "f.close()\n",
    "\n",
    "file_a_contents = load_pickle_file('test_pickles/test_get_one_file.pickle')\n",
    "\n",
    "print(file_a_contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
