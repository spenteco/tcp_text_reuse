{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find Bible Quotation\n",
    "\n",
    "Search all of EEBO-TCP for places where some version of the Bible is quoted.  Save the results in a useful form for later analysis.\n",
    "\n",
    "Here, I'm using three bibles: the EEBO_TCP transcriptions of the Geneva, and of the King James Version; and an Oxford Text Archive copy of the KJV.\n",
    "\n",
    "Most of the logic for this process is packed into matching_functions.py, so this notebook is more or less just a method for queuing up matching tasks, and for starting workers to handle those tasks.\n",
    "\n",
    "The procoess will match 3 bibles, verse by verse, against all of EEBO-TCP in three or four hours on a fairly generic Dell workstation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Magic Number(s)\n",
    "\n",
    "It's my habit to try to put all constant declarations and imports at the top of notebooks.  Usually, I don't comment on them.\n",
    "\n",
    "Here, however, a word about **MAX_GAP_ALLOWED**: quotation finding works by finding matching three lemma sequences, and then by merging overlapping matches.  For example, we may have a matching lemma sequences with starting and ending locations like\n",
    "\n",
    "    [10, 12], [11, 13], [12, 14], [13, 15]\n",
    "    \n",
    "which we'd want to merge as\n",
    "\n",
    "    [10, 15]\n",
    "\n",
    "However, there's enough noise in the data (gaps, slightly different phrasing, misquotation), that we need some way to not let such noise cause us to overlook actual overlap. For example, we might have matches like\n",
    "\n",
    "    [10, 12], [13, 15]\n",
    "\n",
    "which, because they do not overlap, wouldn't merge.  However, **MAX_GAP_ALLOWED** specifies how much tolerance we're willing to allow in finding overlaps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time, pickle, glob, os\n",
    "from queue import Queue\n",
    "from threading import Thread\n",
    "from matching_functions import *\n",
    "\n",
    "BIBLE_SHINGLE_FOLDER = '/home/spenteco/text_reuse.HOME/bible_pickles/'\n",
    "EEBO_SHINGLE_FOLDER = '/home/spenteco/0/eebo_shingled/'\n",
    "RESULTS_FOLDER = '/home/spenteco/0/bible_matches/'\n",
    "\n",
    "NUM_WORKER_THREADS = 6\n",
    "MAX_GAP_ALLOWED = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the Worker Function\n",
    "\n",
    "Really, not much to see here: it's mostly input and output wrapped around a call to the match_two_files function found in matching_functions.py.\n",
    "\n",
    "A couple of things to note:\n",
    "\n",
    "1.  Workers share a global variable, BIBLE_PICKLE_DATA, which is a list of the verses in each of the three versions of the Bible this runs against (~99,000 verses, ~33,000 from each of three bibles), each verse of which is treated as a separate file.\n",
    "\n",
    "2.  When I use the match_two_files function to compare two full EEBO_TCP texts, and I pass in a MIN_MATCH_LENGTH variable which specifies how the merged lemma sequences should be in order to be considered a match.  Here, I do that; however, for each verse, I compute a MIN_MATCH_LENGTH value appropriate for the length of the verse (I wouldn't want a MIN_MATCH_LENGTH value longer than the verse!)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BIBLE_PICKLE_DATA = []\n",
    "all_the_results = []\n",
    "\n",
    "def find_quotation_worker(worker_number):\n",
    "    \n",
    "    while not q.empty():\n",
    "        \n",
    "        result_message = None\n",
    "        \n",
    "        start_time = time.time()\n",
    "        \n",
    "        path_to_eebo_pickle = q.get()\n",
    "        \n",
    "        if os.path.isfile(RESULTS_FOLDER + path_to_eebo_pickle.split('/')[-1]):\n",
    "            pass\n",
    "        else:\n",
    "        \n",
    "            worker_start_time = time.time()\n",
    "        \n",
    "            eebo_data = load_pickle_file(path_to_eebo_pickle)\n",
    "\n",
    "            all_verses_results = []\n",
    "\n",
    "            for bible_verse in BIBLE_PICKLE_DATA:\n",
    "\n",
    "                bible_file = bible_verse[0]\n",
    "                verse_reference = bible_verse[1]['reference']\n",
    "                    \n",
    "                MIN_MATCH_LENGTH = int(len(bible_verse[1]['non_space_lemmas']) * 0.75)\n",
    "        \n",
    "                results = match_two_files(bible_verse[1], \n",
    "                                            eebo_data,\n",
    "                                            MAX_GAP_ALLOWED, MIN_MATCH_LENGTH,\n",
    "                                            return_match_offsets=True)\n",
    "\n",
    "                for r in results:\n",
    "                    all_verses_results.append([bible_file, verse_reference, \n",
    "                                               ''.join(bible_verse[1]['tokens']), r])\n",
    "            \n",
    "            f = open(RESULTS_FOLDER + path_to_eebo_pickle.split('/')[-1], 'wb')\n",
    "            pickle.dump(all_verses_results, f)\n",
    "            f.close()\n",
    "                    \n",
    "            worker_stop_time = time.time()\n",
    "        \n",
    "        q.task_done()\n",
    "        \n",
    "        if result_message != None:\n",
    "            print(result_message + '\\n\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load up the Queue\n",
    "\n",
    "The task queue is just a list of files in EEBO-TCP.\n",
    "\n",
    "I also read Bibles and load BIBLE_PICKLE_DATA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q = Queue()\n",
    "\n",
    "for path_to_eebo in glob.glob(EEBO_SHINGLE_FOLDER + '*.pickle'):\n",
    "    q.put(path_to_eebo)\n",
    "    \n",
    "for path_to_bible in glob.glob(BIBLE_SHINGLE_FOLDER + '*.pickle'):\n",
    "    for v in load_pickle_file(path_to_bible):\n",
    "        \n",
    "        BIBLE_PICKLE_DATA.append([path_to_bible, v])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start Workers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "            \n",
    "    \n",
    "for a in range(NUM_WORKER_THREADS):\n",
    "    t = Thread(target=find_quotation_worker, args=(a,))\n",
    "    t.daemon = True\n",
    "    t.start()\n",
    "\n",
    "q.join()\n",
    "\n",
    "print('time:', (time.time() - start_time))\n",
    "\n",
    "print('Done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
