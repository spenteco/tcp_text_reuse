{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How matching works\n",
    "\n",
    "\n",
    "### First, some preliminary preprocessing\n",
    "\n",
    "There's a better example of this in **test_matching_functions_part_1.ipynb**; for now, let's accept it as a given . . . "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matching_functions import *\n",
    "\n",
    "SHINGLE_LENGTH = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done!\n"
     ]
    }
   ],
   "source": [
    "for file_name in ['sample_a', 'sample_b']:\n",
    "    \n",
    "    file_data = preprocess_one_file('test_adorned_xml/' + file_name + '.xml', SHINGLE_LENGTH)\n",
    "\n",
    "    f = open('test_pickles/' + file_name + '.pickle', 'wb')\n",
    "    pickle.dump(file_data, f)\n",
    "    f.close()\n",
    "    \n",
    "print('Done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_a = load_pickle_file('test_pickles/sample_a.pickle')\n",
    "sample_b = load_pickle_file('test_pickles/sample_b.pickle')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The results of preprocessing . . . \n",
    "\n",
    "We have two very simple texts, which consist of single-character \"words\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "B C G H * * * B C G H F J K L * * * F J K L\n",
      "\n",
      "B C G H F J K L\n"
     ]
    }
   ],
   "source": [
    "print(' '.join(sample_a['tokens']))\n",
    "print()\n",
    "print(' '.join(sample_b['tokens']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The pre-processed texts consist of . . . \n",
    "\n",
    " . . . five collections of data:\n",
    " \n",
    " 1.  **tokens** which are all the words and punctuation from the source texts (which have been morphadorned).\n",
    " 2.  **lemmas**; note, however, we replace a number of lemma values (punctuation, stop words, words with non-latin characters, numbers) with spaces.  Note that in this example, the \"\\*\" tokens in sample_a are replaced with space in lemmas.  We also lower-case at this point.\n",
    " 3.  **non_space_lemmas** which has the same values as lemmas, except that we've dropped all the spaces.\n",
    " 4.  **offsets** which indicate the position of each non_space_lemma within the lists of lemmas and tokens.\n",
    " 5.  **shingles**, which are ngrams made from non_space_lemmas; each ngram us associated with its starting and ending position in offsets.\n",
    " \n",
    "\n",
    "Here's a very simple example of a text with five one-letter \"words\": \n",
    " \n",
    "     tokens: \\['B', 'C', '*', 'G', 'H'\\]\n",
    "     lemmas: \\['b', 'c', ' ', 'g', 'h'\\]\n",
    "     non_space_lemmas: \\['b', 'c', 'g', 'h'\\]\n",
    "     offsets: \\[0, 1, 3, 4\\]\n",
    "     shingles: {('b', 'c', 'g'): \\[\\[0, 2\\]\\], ('c', 'g', 'h'): \\[\\[1, 3\\]\\]} \n",
    "     \n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'tokens': ['B', 'C', 'G', 'H', '*', '*', '*', 'B', 'C', 'G', 'H', 'F', 'J', 'K', 'L', '*', '*', '*', 'F', 'J', 'K', 'L'], 'lemmas': ['b', 'c', 'g', 'h', ' ', ' ', ' ', 'b', 'c', 'g', 'h', 'f', 'j', 'k', 'l', ' ', ' ', ' ', 'f', 'j', 'k', 'l'], 'non_space_lemmas': ['b', 'c', 'g', 'h', 'b', 'c', 'g', 'h', 'f', 'j', 'k', 'l', 'f', 'j', 'k', 'l'], 'offsets': [0, 1, 2, 3, 7, 8, 9, 10, 11, 12, 13, 14, 18, 19, 20, 21], 'shingles': {('b', 'c', 'g'): [[0, 2], [4, 6]], ('c', 'g', 'h'): [[1, 3], [5, 7]], ('g', 'h', 'b'): [[2, 4]], ('h', 'b', 'c'): [[3, 5]], ('g', 'h', 'f'): [[6, 8]], ('h', 'f', 'j'): [[7, 9]], ('f', 'j', 'k'): [[8, 10], [12, 14]], ('j', 'k', 'l'): [[9, 11], [13, 15]], ('k', 'l', 'f'): [[10, 12]], ('l', 'f', 'j'): [[11, 13]]}}\n",
      "\n",
      "{'tokens': ['B', 'C', 'G', 'H', 'F', 'J', 'K', 'L'], 'lemmas': ['b', 'c', 'g', 'h', 'f', 'j', 'k', 'l'], 'non_space_lemmas': ['b', 'c', 'g', 'h', 'f', 'j', 'k', 'l'], 'offsets': [0, 1, 2, 3, 4, 5, 6, 7], 'shingles': {('b', 'c', 'g'): [[0, 2]], ('c', 'g', 'h'): [[1, 3]], ('g', 'h', 'f'): [[2, 4]], ('h', 'f', 'j'): [[3, 5]], ('f', 'j', 'k'): [[4, 6]], ('j', 'k', 'l'): [[5, 7]]}}\n"
     ]
    }
   ],
   "source": [
    "print(sample_a)\n",
    "print()\n",
    "print(sample_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "a to b ------------------------------\n",
      "\n",
      "BCGH <--> BCGH\n",
      "BCGHFJKL <--> BCGHFJKL\n",
      "FJKL <--> FJKL\n",
      "\n",
      "b to a ------------------------------\n",
      "\n",
      "BCGH <--> BCGH\n",
      "BCGHFJKL <--> BCGHFJKL\n",
      "FJKL <--> FJKL\n"
     ]
    }
   ],
   "source": [
    "MAX_GAP_ALLOWED = 1\n",
    "MIN_MATCH_LENGTH = 4\n",
    "\n",
    "results = match_two_files(sample_a, \n",
    "                            sample_b,\n",
    "                            MAX_GAP_ALLOWED, \n",
    "                            MIN_MATCH_LENGTH)\n",
    "\n",
    "print()\n",
    "print('a to b ------------------------------')\n",
    "print()\n",
    "\n",
    "for fn, f in enumerate(results):\n",
    "    print(f[0], '<-->', f[1])\n",
    "\n",
    "results = match_two_files(sample_b, \n",
    "                            sample_a,\n",
    "                            MAX_GAP_ALLOWED, \n",
    "                            MIN_MATCH_LENGTH)\n",
    "\n",
    "print()\n",
    "print('b to a ------------------------------')\n",
    "print()\n",
    "\n",
    "for fn, f in enumerate(results):\n",
    "    print(f[0], '<-->', f[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample_a shingles {('b', 'c', 'g'): [[0, 2], [4, 6]], ('c', 'g', 'h'): [[1, 3], [5, 7]], ('g', 'h', 'b'): [[2, 4]], ('h', 'b', 'c'): [[3, 5]], ('g', 'h', 'f'): [[6, 8]], ('h', 'f', 'j'): [[7, 9]], ('f', 'j', 'k'): [[8, 10], [12, 14]], ('j', 'k', 'l'): [[9, 11], [13, 15]], ('k', 'l', 'f'): [[10, 12]], ('l', 'f', 'j'): [[11, 13]]}\n",
      "\n",
      "sample_b shingles {('b', 'c', 'g'): [[0, 2]], ('c', 'g', 'h'): [[1, 3]], ('g', 'h', 'f'): [[2, 4]], ('h', 'f', 'j'): [[3, 5]], ('f', 'j', 'k'): [[4, 6]], ('j', 'k', 'l'): [[5, 7]]}\n",
      "\n",
      "match [[0, 2], [0, 2]]\n",
      "match [[1, 3], [1, 3]]\n",
      "match [[4, 6], [0, 2]]\n",
      "match [[5, 7], [1, 3]]\n",
      "match [[6, 8], [2, 4]]\n",
      "match [[7, 9], [3, 5]]\n",
      "match [[8, 10], [4, 6]]\n",
      "match [[9, 11], [5, 7]]\n",
      "match [[12, 14], [4, 6]]\n",
      "match [[13, 15], [5, 7]]\n"
     ]
    }
   ],
   "source": [
    "print('sample_a shingles', sample_a['shingles'])\n",
    "print()\n",
    "\n",
    "print('sample_b shingles', sample_b['shingles'])\n",
    "print()\n",
    "\n",
    "matches = []\n",
    "for k in sample_a['shingles'].keys():\n",
    "    if k in sample_b['shingles']:\n",
    "        for v_a in sample_a['shingles'][k]:\n",
    "            for v_b in sample_b['shingles'][k]:\n",
    "                matches.append([v_a, v_b])\n",
    "                \n",
    "matches.sort()\n",
    "\n",
    "for m in matches:\n",
    "    print('match', m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "grouped_match [[[0, 2], [0, 2]], [[1, 3], [1, 3]]]\n",
      "grouped_match [[[4, 6], [0, 2]], [[5, 7], [1, 3]], [[6, 8], [2, 4]], [[7, 9], [3, 5]], [[8, 10], [4, 6]], [[9, 11], [5, 7]]]\n",
      "grouped_match [[[12, 14], [4, 6]], [[13, 15], [5, 7]]]\n"
     ]
    }
   ],
   "source": [
    "grouped_matches = [[matches[0],],]\n",
    "\n",
    "for m in matches[1:]:\n",
    "\n",
    "    group_match_n = -1\n",
    "    for gn, g in enumerate(grouped_matches):\n",
    "        if (m[0][0] - MAX_GAP_ALLOWED) < g[-1][0][1] and \\\n",
    "            (m[1][0] - MAX_GAP_ALLOWED) < g[-1][1][1] and \\\n",
    "            m[0][0] > g[-1][0][0] and \\\n",
    "            m[1][0] > g[-1][1][0]:\n",
    "\n",
    "            group_match_n = gn\n",
    "            break\n",
    "\n",
    "    if group_match_n > -1:\n",
    "        grouped_matches[gn].append(m)\n",
    "    else:\n",
    "        grouped_matches.append([m])\n",
    "\n",
    "for m in grouped_matches:\n",
    "    print('grouped_match', m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "merged_match [[0, 3], [0, 3]]\n",
      "merged_match [[4, 11], [0, 7]]\n",
      "merged_match [[12, 15], [4, 7]]\n"
     ]
    }
   ],
   "source": [
    "MIN_MATCH_LENGTH = 4\n",
    "\n",
    "merged_matches = []\n",
    "for gn, g in enumerate(grouped_matches):\n",
    "\n",
    "    from_matches = []\n",
    "    to_matches = []\n",
    "    for m in g:\n",
    "        from_matches.append(m[0])\n",
    "        to_matches.append(m[1])\n",
    "\n",
    "    from_matches.sort()\n",
    "    to_matches.sort()\n",
    "\n",
    "    if from_matches[-1][1] - from_matches[0][0] >= MIN_MATCH_LENGTH - 1: \n",
    "\n",
    "        merged_matches.append([[from_matches[0][0], from_matches[-1][1]], \n",
    "                                [to_matches[0][0], to_matches[-1][1]]])\n",
    "\n",
    "for m in merged_matches:\n",
    "    print('merged_match', m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "merged_match [[0, 3], [0, 3]]\n",
      "\tsample_a offsets 0 3\n",
      "\tsample_b offsets 0 3\n",
      "\n",
      "merged_match [[4, 11], [0, 7]]\n",
      "\tsample_a offsets 7 14\n",
      "\tsample_b offsets 0 7\n",
      "\n",
      "merged_match [[12, 15], [4, 7]]\n",
      "\tsample_a offsets 18 21\n",
      "\tsample_b offsets 4 7\n",
      "\n",
      "final_result ['BCGH', 'BCGH']\n",
      "final_result ['BCGHFJKL', 'BCGHFJKL']\n",
      "final_result ['FJKL', 'FJKL']\n"
     ]
    }
   ],
   "source": [
    "final_results = []\n",
    "\n",
    "for m in merged_matches:\n",
    "    \n",
    "    print()\n",
    "    print('merged_match', m)\n",
    "\n",
    "    a_from_offset = sample_a['offsets'][m[0][0]]\n",
    "    a_to_offset = sample_a['offsets'][m[0][1]]\n",
    "    \n",
    "    print('\\tsample_a offsets', a_from_offset, a_to_offset)\n",
    "\n",
    "    b_from_offset = sample_b['offsets'][m[1][0]]\n",
    "    b_to_offset = sample_b['offsets'][m[1][1]]\n",
    "    \n",
    "    print('\\tsample_b offsets', b_from_offset, b_to_offset)\n",
    "\n",
    "    final_results.append([''.join(sample_a['tokens'][a_from_offset: a_to_offset + 1]),\n",
    "                                ''.join(sample_b['tokens'][b_from_offset: b_to_offset + 1])])\n",
    "    \n",
    "print()\n",
    "for r in final_results:\n",
    "    print('final_result', r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
